{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8c348c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb68a147",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(object):\n",
    "    \n",
    "    def forward(x, w, b):\n",
    "        \"\"\"\n",
    "        Computes the forward pass for a linear (fully-connected) layer.\n",
    "        The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N\n",
    "        examples, where each example x[i] has shape (d_1, ..., d_k). We will\n",
    "        reshape each input into a vector of dimension D = d_1 * ... * d_k, and\n",
    "        then transform it to an output vector of dimension M.\n",
    "        Inputs:\n",
    "        - x: A tensor containing input data, of shape (N, d_1, ..., d_k)\n",
    "        - w: A tensor of weights, of shape (D, M)\n",
    "        - b: A tensor of biases, of shape (M,)\n",
    "        Returns a tuple of:\n",
    "        - out: output, of shape (N, M)\n",
    "        - cache: (x, w, b)\n",
    "        \"\"\"\n",
    "        # Reshape x into rows\n",
    "        N = x.shape[0] # Number of examples\n",
    "        x_row = x.reshape(N, -1) # Reshape x into rows of size D\n",
    "        D = x_row.shape[1] # Dimension of input\n",
    "        M = b.shape[0] # Dimension of output\n",
    "    \n",
    "        # Compute out\n",
    "        out = x_row.mm(w) + b # Matrix multiplication and broadcasted addition\n",
    "    \n",
    "        # Store cache\n",
    "        cache = (x, w, b)\n",
    "    \n",
    "        return out, cache\n",
    "    \n",
    "    def backward(dout, cache):\n",
    "        \"\"\"\n",
    "        Computes the backward pass for a linear layer.\n",
    "        Inputs:\n",
    "        - dout: Upstream derivative, of shape (N, M)\n",
    "        - cache: Tuple of:\n",
    "          - x: Input data, of shape (N, d_1, ... d_k)\n",
    "          - w: Weights, of shape (D, M)\n",
    "          - b: Biases, of shape (M,)\n",
    "    \n",
    "        Returns a tuple of:\n",
    "        - dx: Gradient with respect to x, of shape\n",
    "          (N, d1, ..., d_k)\n",
    "        - dw: Gradient with respect to w, of shape (D, M)\n",
    "        - db: Gradient with respect to b, of shape (M,)\n",
    "        \"\"\"\n",
    "        # Unpack cache\n",
    "        x, w, b = cache\n",
    "    \n",
    "        # Reshape x into rows\n",
    "        N = x.shape[0] # Number of examples\n",
    "        x_row = x.reshape(N, -1) # Reshape x into rows of size D\n",
    "        D = x_row.shape[1] # Dimension of input\n",
    "        M = b.shape[0] # Dimension of output\n",
    "    \n",
    "        # Compute gradients\n",
    "        dx = dout.mm(w.t()).reshape(x.shape) # Reshape back to original shape of x\n",
    "        dw = x_row.t().mm(dout) # Matrix multiplication\n",
    "        db = dout.sum(dim=0) # Sum over rows\n",
    "    \n",
    "        return dx, dw, db\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b572da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(object):\n",
    "\n",
    "    def forward(x):\n",
    "        \"\"\"\n",
    "        Computes the forward pass for a layer of rectified\n",
    "        linear units (ReLUs).\n",
    "    \n",
    "        Input:\n",
    "        - x: Input; a tensor of any shape\n",
    "    \n",
    "        Returns a tuple of:\n",
    "        - out: Output, a tensor of the same shape as x\n",
    "        - cache: x\n",
    "        \"\"\"\n",
    "        out = torch.relu(x)\n",
    "        cache = x\n",
    "        return out, cache\n",
    "    \n",
    "    def backward(dout, cache):\n",
    "        \"\"\"\n",
    "        Computes the backward pass for a layer of rectified\n",
    "        linear units (ReLUs).\n",
    "\n",
    "        Input:\n",
    "        - dout: Upstream derivatives, of any shape\n",
    "        - cache: Input x, of same shape as dout\n",
    "\n",
    "        Returns:\n",
    "        - dx: Gradient with respect to x\n",
    "        \"\"\"\n",
    "        dx = dout * (cache > 0)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87febb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_ReLU(object):\n",
    "    \n",
    "    def forward(x, w, b):\n",
    "        # Convenience layer that performs an linear transform\n",
    "        # followed by a ReLU.\n",
    "        # Inputs:\n",
    "        # - x: Input to the linear layer\n",
    "        # - w, b: Weights for the linear layer\n",
    "        # Returns a tuple of:\n",
    "        # - out: Output from the ReLU\n",
    "        # - cache: Object to give to the backward pass (hint: cache = (fc_cache, relu_cache))\n",
    "\n",
    "        # Reshape x to a matrix of shape (2, 12)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # Compute the linear transform\n",
    "        out = x.mm(w) + b\n",
    "\n",
    "        # Save the cache for the backward pass\n",
    "        fc_cache = (x, w, b)\n",
    "\n",
    "        # Apply the ReLU activation\n",
    "        out = torch.relu(out)\n",
    "\n",
    "        # Save the cache for the backward pass\n",
    "        relu_cache = out\n",
    "\n",
    "        # Return the output and the cache\n",
    "        cache = (fc_cache, relu_cache)\n",
    "        return out, cache\n",
    "\n",
    "\n",
    "\n",
    "    def backward(dout, cache):\n",
    "        # Backward pass for the linear-relu convenience layer\n",
    "        # Inputs:\n",
    "        # - dout: Upstream derivatives\n",
    "        # - cache: Tuple of (fc_cache, relu_cache) from forward pass\n",
    "        # Returns a tuple of:\n",
    "        # - dx: Gradient with respect to x\n",
    "        # - dw: Gradient with respect to w\n",
    "        # - db: Gradient with respect to b\n",
    "\n",
    "        # Unpack the cache\n",
    "        fc_cache, relu_cache = cache\n",
    "\n",
    "        # Compute the gradient of the ReLU activation\n",
    "        dout = dout * (relu_cache > 0)\n",
    "\n",
    "        # Unpack the fc_cache\n",
    "        x, w, b = fc_cache\n",
    "\n",
    "        # Compute the gradient of the linear transform\n",
    "        dx = dout.mm(w.t())\n",
    "        dw = x.t().mm(dout)\n",
    "        db = dout.sum(dim=0)\n",
    "        \n",
    "        dx = dx.view([2, -1, 4])\n",
    "\n",
    "        # Return the gradients\n",
    "        return dx, dw, db\n",
    "    \n",
    "    def softmax_loss(x, y):\n",
    "        \"\"\"\n",
    "        Computes the loss and gradient for softmax classification.\n",
    "        Inputs:\n",
    "        - x: Input data, of shape (N, C) where x[i, j] is the score for\n",
    "          the jth class for the ith input.\n",
    "        - y: Vector of labels, of shape (N,) where y[i] is the label\n",
    "          for x[i] and 0 <= y[i] < C\n",
    "        Returns a tuple of:\n",
    "        - loss: Scalar giving the loss\n",
    "        - dx: Gradient of the loss with respect to x\n",
    "        \"\"\"\n",
    "        # Get the number of samples\n",
    "        N = x.shape[0]\n",
    "\n",
    "        # Compute the softmax scores\n",
    "        scores = torch.exp(x - x.max(dim=1, keepdim=True)[0])\n",
    "        probs = scores / scores.sum(dim=1, keepdim=True)\n",
    "\n",
    "        # Compute the cross-entropy loss\n",
    "        loss = -torch.log(probs[torch.arange(N), y]).mean()\n",
    "\n",
    "        # Compute the gradient of the loss with respect to x\n",
    "        dx = probs.clone()\n",
    "        dx[torch.arange(N), y] -= 1\n",
    "        dx /= N\n",
    "\n",
    "        # Return the loss and gradient\n",
    "        return loss, dx\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25fded24",
   "metadata": {},
   "outputs": [],
   "source": [
    " def softmax_loss(x, y):\n",
    "        \"\"\"\n",
    "        Computes the loss and gradient for softmax classification.\n",
    "        Inputs:\n",
    "        - x: Input data, of shape (N, C) where x[i, j] is the score for\n",
    "          the jth class for the ith input.\n",
    "        - y: Vector of labels, of shape (N,) where y[i] is the label\n",
    "          for x[i] and 0 <= y[i] < C\n",
    "        Returns a tuple of:\n",
    "        - loss: Scalar giving the loss\n",
    "        - dx: Gradient of the loss with respect to x\n",
    "        \"\"\"\n",
    "        # Get the number of samples\n",
    "        N = x.shape[0]\n",
    "\n",
    "        # Compute the softmax scores\n",
    "        scores = torch.exp(x - x.max(dim=1, keepdim=True)[0])\n",
    "        probs = scores / scores.sum(dim=1, keepdim=True)\n",
    "\n",
    "        # Compute the cross-entropy loss\n",
    "        loss = -torch.log(probs[torch.arange(N), y]).mean()\n",
    "\n",
    "        # Compute the gradient of the loss with respect to x\n",
    "        dx = probs.clone()\n",
    "        dx[torch.arange(N), y] -= 1\n",
    "        dx /= N\n",
    "\n",
    "        # Return the loss and gradient\n",
    "        return loss, dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f77dbe83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dscores, h, W2, b2):\n",
    "    # Compute the gradient of the loss with respect to the input of the linear layer\n",
    "    dh = dscores.mm(W2.t())\n",
    "    # Compute the gradient of the loss with respect to the weight matrix of the linear layer\n",
    "    dW2 = h.t().mm(dscores)\n",
    "    # Compute the gradient of the loss with respect to the bias vector of the linear layer\n",
    "    db2 = dscores.sum(dim=0)\n",
    "    # Return the gradients\n",
    "    return dh, dW2, db2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "637ae1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dout, cache):\n",
    "    dx = dout * (cache > 0)\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07e7827e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class TwoLayerNet(object):\n",
    "\n",
    "    def __init__(self, input_dim=3*32*32, hidden_dim=100, num_classes=10,weight_scale=1e-3, reg=0.0,dtype=torch.float32,\n",
    "                device='cpu'):\n",
    "        self.params = {}\n",
    "        self.reg = reg\n",
    "        # Initialize the first layer weights and biases\n",
    "        self.params['W1'] = torch.randn(input_dim, hidden_dim, dtype=dtype, device=device) * weight_scale\n",
    "        self.params['b1'] = torch.zeros(hidden_dim, dtype=dtype, device=device)\n",
    "        # Initialize the second layer weights and biases\n",
    "        self.params['W2'] = torch.randn(hidden_dim, num_classes, dtype=dtype, device=device) * weight_scale\n",
    "        self.params['b2'] = torch.zeros(num_classes, dtype=dtype, device=device)\n",
    "\n",
    "    def save(self, path):\n",
    "        checkpoint = {\n",
    "          'reg': self.reg,\n",
    "          'params': self.params,\n",
    "        }\n",
    "        torch.save(checkpoint, path)\n",
    "        print(\"Saved in {}\".format(path))\n",
    "        \n",
    "    def load(self, path, dtype, device):\n",
    "        checkpoint = torch.load(path, map_location='device')\n",
    "        self.params = checkpoint['params']\n",
    "        self.reg = checkpoint['reg']\n",
    "        for p in self.params:\n",
    "            self.params[p] = self.params[p].type(dtype).to(device)\n",
    "        print(\"load checkpoint file: {}\".format(path))\n",
    "        \n",
    "    def loss(self, X, y=None):\n",
    "\n",
    "        scores = None\n",
    "\n",
    "        # Unpack the parameters\n",
    "        W1, b1 = self.params['W1'], self.params['b1']\n",
    "        W2, b2 = self.params['W2'], self.params['b2']\n",
    "\n",
    "        # Forward pass\n",
    "        # Compute the hidden layer\n",
    "        h = X.mm(W1) + b1\n",
    "        # Apply the ReLU activation\n",
    "        h = torch.relu(h)\n",
    "        # Compute the output layer\n",
    "        scores = h.mm(W2) + b2\n",
    "\n",
    "        # If y is None, we are in test mode, so just return scores\n",
    "        if y is None:\n",
    "            return scores\n",
    "    \n",
    "        loss, grads = 0, {}\n",
    "\n",
    "        # Compute the softmax loss\n",
    "        loss, dscores = softmax_loss(scores, y)\n",
    "\n",
    "        # Add the L2 regularization term\n",
    "        reg = self.reg\n",
    "        loss += reg * (torch.sum(W1 ** 2) + torch.sum(W2 ** 2))\n",
    "\n",
    "        # Backward pass\n",
    "        grads = {}\n",
    "        # Compute the gradient of the output layer\n",
    "        dh, dW2, db2 = linear_backward(dscores, h, W2, b2)\n",
    "        # Add the L2 regularization term\n",
    "        dW2 += 2 * reg * W2\n",
    "        # Compute the gradient of the ReLU activation\n",
    "        dh = relu_backward(dh, h)\n",
    "        # Compute the gradient of the hidden layer\n",
    "        dx, dW1, db1 = linear_backward(dh, X, W1, b1)\n",
    "        # Add the L2 regularization term\n",
    "        dW1 += 2 * reg * W1\n",
    "\n",
    "        # Store the gradients in the dictionary\n",
    "        grads['W1'] = dW1\n",
    "        grads['b1'] = db1\n",
    "        grads['W2'] = dW2\n",
    "        grads['b2'] = db2\n",
    "\n",
    "        return loss, grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3297ccd",
   "metadata": {},
   "source": [
    "# ##################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "707a26df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_relu_forward(x, w, b):\n",
    "    \"\"\"Convenience layer that performs an affine transform followed by a ReLU.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input to the affine layer\n",
    "    - w, b: Weights for the affine layer\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output from the ReLU\n",
    "    - cache: Object to give to the backward pass\n",
    "    \"\"\"\n",
    "    a, fc_cache = affine_forward(x, w, b)\n",
    "    out, relu_cache = relu_forward(a)\n",
    "    cache = (fc_cache, relu_cache)\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7e1cd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_relu_backward(dout, cache):\n",
    "    \"\"\"Backward pass for the affine-relu convenience layer.\n",
    "    \"\"\"\n",
    "    fc_cache, relu_cache = cache\n",
    "    da = relu_backward(dout, relu_cache)\n",
    "    dx, dw, db = affine_backward(da, fc_cache)\n",
    "    return dx, dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1bacfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def affine_forward(x, w, b):\n",
    "\n",
    "    out = None\n",
    "    N = x.shape[0] # Number of examples\n",
    "    x_row = torch.reshape(x, (N, -1)) # Reshape x into rows of size D\n",
    "    out = x_row.matmul(w) + b\n",
    "    cache = (x, w, b)\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54767e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def affine_backward(dout, cache):\n",
    "\n",
    "    x, w, b = cache\n",
    "    dx, dw, db = None, None, None\n",
    "    pass\n",
    "    dx = dout.matmul(w.T)\n",
    "    dx = torch.reshape(dx, (dx.shape[0], *x.shape[1:]))\n",
    "    t = torch.reshape(x, (x.shape[0], -1))\n",
    "    dw = t.T.matmul(dout)\n",
    "    db = torch.sum(dout, dim=0) # Sum over rows\n",
    "    return dx, dw, db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11235ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def relu_forward(x):\n",
    "    out = None\n",
    "    pass\n",
    "    out = torch.where(x > 0, x, 0)\n",
    "    cache = x\n",
    "    return out, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5afa100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def relu_backwardd(dout, cache):\n",
    "    \n",
    "    dx, x = None, cache\n",
    "    pass\n",
    "    dx = torch.where(x > 0, dout, 0)\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b7fbe1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def batchnorm_forward(x, gamma, beta, bn_param):\n",
    "    \"\"\"Forward pass for batch normalization.\n",
    "\n",
    "    During training the sample mean and (uncorrected) sample variance are\n",
    "    computed from minibatch statistics and used to normalize the incoming data.\n",
    "    During training we also keep an exponentially decaying running mean of the\n",
    "    mean and variance of each feature, and these averages are used to normalize\n",
    "    data at test-time.\n",
    "\n",
    "    At each timestep we update the running averages for mean and variance using\n",
    "    an exponential decay based on the momentum parameter:\n",
    "\n",
    "    running_mean = momentum * running_mean + (1 - momentum) * sample_mean\n",
    "    running_var = momentum * running_var + (1 - momentum) * sample_var\n",
    "\n",
    "    Note that the batch normalization paper suggests a different test-time\n",
    "    behavior: they compute sample mean and variance for each feature using a\n",
    "    large number of training images rather than using a running average. For\n",
    "    this implementation we have chosen to use running averages instead since\n",
    "    they do not require an additional estimation step; the torch7\n",
    "    implementation of batch normalization also uses running averages.\n",
    "\n",
    "    Input:\n",
    "    - x: Data of shape (N, D)\n",
    "    - gamma: Scale parameter of shape (D,)\n",
    "    - beta: Shift paremeter of shape (D,)\n",
    "    - bn_param: Dictionary with the following keys:\n",
    "      - mode: 'train' or 'test'; required\n",
    "      - eps: Constant for numeric stability\n",
    "      - momentum: Constant for running mean / variance.\n",
    "      - running_mean: Array of shape (D,) giving running mean of features\n",
    "      - running_var Array of shape (D,) giving running variance of features\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: of shape (N, D)\n",
    "    - cache: A tuple of values needed in the backward pass\n",
    "    \"\"\"\n",
    "    mode = bn_param[\"mode\"]\n",
    "    eps = bn_param.get(\"eps\", 1e-5)\n",
    "    momentum = bn_param.get(\"momentum\", 0.9)\n",
    "\n",
    "    N, D = x.shape \n",
    "    running_mean = bn_param.get(\"running_mean\", torch.zeros(D, dtype=x.dtype)) \n",
    "    running_var = bn_param.get(\"running_var\", torch.zeros(D, dtype=x.dtype))\n",
    "   \n",
    "    out, cache = None, None\n",
    "    if mode == \"train\":\n",
    "\n",
    "\n",
    "        pass\n",
    "        cache = {}\n",
    "        sample_mean = torch.mean(x, dim=0, keepdim=True) \n",
    "        sample_var = torch.var(x, dim=0, keepdim=True)\n",
    "        x_hat = (x-sample_mean) / torch.sqrt(sample_var + eps)\n",
    "        out = gamma * x_hat + beta\n",
    "\n",
    "        running_mean = momentum * running_mean + (1-momentum) * sample_mean\n",
    "        running_var = momentum * running_var + (1-momentum) * sample_var\n",
    "        cache['x_hat'] = x_hat\n",
    "        cache['eps'] = eps\n",
    "        cache['var'] = sample_var\n",
    "        cache['x'] = x\n",
    "        cache['mean'] = sample_mean\n",
    "        cache['gamma'] = gamma\n",
    "\n",
    "        \n",
    "    elif mode == \"test\":\n",
    "        \n",
    "\n",
    "        pass\n",
    "        x_hat = (x-running_mean) / torch.sqrt(running_var+eps)\n",
    "        out = gamma * x_hat + beta\n",
    "\n",
    "        \n",
    "    else:\n",
    "        raise ValueError('Invalid forward batchnorm mode \"%s\"' % mode)\n",
    "\n",
    "    # Store the updated running means back into bn_param\n",
    "    bn_param[\"running_mean\"] = running_mean\n",
    "    bn_param[\"running_var\"] = running_var\n",
    "\n",
    "    return out, cache\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23f55ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def batchnorm_backward(dout, cache):\n",
    "    \"\"\"Backward pass for batch normalization.\n",
    "\n",
    "    For this implementation, you should write out a computation graph for\n",
    "    batch normalization on paper and propagate gradients backward through\n",
    "    intermediate nodes.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivatives, of shape (N, D)\n",
    "    - cache: Variable of intermediates from batchnorm_forward.\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to inputs x, of shape (N, D)\n",
    "    - dgamma: Gradient with respect to scale parameter gamma, of shape (D,)\n",
    "    - dbeta: Gradient with respect to shift parameter beta, of shape (D,)\n",
    "    \"\"\"\n",
    "    dx, dgamma, dbeta = None, None, None\n",
    "    \n",
    "  \n",
    "    pass\n",
    "    N = cache['x'].shape[0]\n",
    "\n",
    "    dgamma = torch.sum(cache['x_hat'] * dout, dim=0)\n",
    "    dbeta = torch.sum(dout, dim=0)\n",
    "\n",
    "    dx_hat = torch.tensor([cache['gamma']]) * dout\n",
    "    dtmp2 = dx_hat * torch.pow(cache['var'] + cache['eps'], -0.5)\n",
    "    dtmp3 = dx_hat * (cache['x'] - cache['mean'])\n",
    "    dx = dtmp2\n",
    "    dmean = torch.sum(-dtmp2, dim=0)\n",
    "    dvar = torch.sum(dtmp3 * (-0.5) * torch.pow(cache['var'] + cache['eps'], -1.5), dim=0)\n",
    "    dx += (cache['x'] - cache['mean']) * 2 / N * dvar\n",
    "    dmean += torch.sum((cache['mean'] - cache['x']) * 2 / N * dvar, dim=0)\n",
    "    dx += dmean / N\n",
    "\n",
    "    \n",
    "\n",
    "    return dx, dgamma, dbeta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a3dc747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def layernorm_forward(x, gamma, beta, ln_param):\n",
    "    \"\"\"Forward pass for layer normalization.\n",
    "\n",
    "    During both training and test-time, the incoming data is normalized per data-point,\n",
    "    before being scaled by gamma and beta parameters identical to that of batch normalization.\n",
    "\n",
    "    Note that in contrast to batch normalization, the behavior during train and test-time for\n",
    "    layer normalization are identical, and we do not need to keep track of running averages\n",
    "    of any sort.\n",
    "\n",
    "    Input:\n",
    "    - x: Data of shape (N, D)\n",
    "    - gamma: Scale parameter of shape (D,)\n",
    "    - beta: Shift paremeter of shape (D,)\n",
    "    - ln_param: Dictionary with the following keys:\n",
    "        - eps: Constant for numeric stability\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: of shape (N, D)\n",
    "    - cache: A tuple of values needed in the backward pass\n",
    "    \"\"\"\n",
    "    out, cache = None, None\n",
    "    eps = ln_param.get(\"eps\", 1e-5)\n",
    "    \n",
    "\n",
    "    pass\n",
    "    cache = {}\n",
    "    xt = x.T\n",
    "    N = xt.shape[0]\n",
    "    mean = torch.sum(xt, axis=0) / N\n",
    "    var = torch.sum((xt - mean) * (xt - mean), axis=0) / N\n",
    "    sqrt = torch.sqrt(var + eps)\n",
    "    x_hat = ((xt - mean) / sqrt).T\n",
    "    out = x_hat * gamma + beta\n",
    "\n",
    "    cache['x_hat'] = x_hat\n",
    "    cache['sqrt'] = sqrt\n",
    "    cache['gamma'] = gamma\n",
    "\n",
    "    \n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e8e0fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def layernorm_backward(dout, cache):\n",
    "    \"\"\"Backward pass for layer normalization.\n",
    "\n",
    "    For this implementation, you can heavily rely on the work you've done already\n",
    "    for batch normalization.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivatives, of shape (N, D)\n",
    "    - cache: Variable of intermediates from layernorm_forward.\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to inputs x, of shape (N, D)\n",
    "    - dgamma: Gradient with respect to scale parameter gamma, of shape (D,)\n",
    "    - dbeta: Gradient with respect to shift parameter beta, of shape (D,)\n",
    "    \"\"\"\n",
    "    dx, dgamma, dbeta = None, None, None\n",
    "    \n",
    "\n",
    "    pass\n",
    "    N = dout.shape[1]\n",
    "    dbeta = torch.sum(dout, axis=0)\n",
    "    dgamma = torch.sum(cache['x_hat'] * dout, axis=0)\n",
    "    dx_hat = (torch.tensor([cache['gamma']]) * dout).T\n",
    "    dx = -cache['x_hat'].T / cache['sqrt'] / N\n",
    "    dx = dx * torch.sum(dx_hat * cache['x_hat'].T, axis=0)\n",
    "    dx -= torch.sum(dx_hat, axis=0) / cache['sqrt'] / N\n",
    "    dx += dx_hat / cache['sqrt']\n",
    "    dx = dx.T\n",
    "\n",
    "    \n",
    "    return dx, dgamma, dbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9a58651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def dropout_forward(x, dropout_param):  \n",
    "    p, mode = dropout_param[\"p\"], dropout_param[\"mode\"]\n",
    "    if \"seed\" in dropout_param:\n",
    "        torch.manual_seed(dropout_param[\"seed\"])  \n",
    "    mask = None\n",
    "    out = None\n",
    "    if mode == \"train\":            \n",
    "        pass\n",
    "        mask = torch.bernoulli(torch.ones(*x.shape) * (1-p)) / (1-p)\n",
    "        out = mask * x      \n",
    "    elif mode == \"test\":\n",
    "        pass\n",
    "        out = x\n",
    "    cache = (dropout_param, mask)\n",
    "    out = out.to(x.dtype)\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ddb6328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def dropout_backward(dout, cache):   \n",
    "    dropout_param, mask = cache\n",
    "    mode = dropout_param[\"mode\"]\n",
    "    dx = None\n",
    "    if mode == \"train\":        \n",
    "        pass\n",
    "        dx = dout * mask\n",
    "    elif mode == \"test\":\n",
    "        dx = dout\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a4e4c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def softmax_loss2(x, y):\n",
    "    \"\"\"Computes the loss and gradient for softmax classification.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth\n",
    "      class for the ith input.\n",
    "    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n",
    "      0 <= y[i] < C\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss: Scalar giving the loss\n",
    "    - dx: Gradient of the loss with respect to x\n",
    "    \"\"\"\n",
    "    loss, dx = None, None\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    pass\n",
    "    eps = 1e-7\n",
    "    num_x = x.shape[0]\n",
    "    exp_score = torch.exp(x)\n",
    "    norm_score = exp_score/torch.sum(exp_score,axis=1).reshape(num_x,-1)\n",
    "    loss = torch.sum(-1*torch.log(norm_score[range(num_x),y]+eps)) / num_x\n",
    "    \n",
    "    dx = norm_score.clone()\n",
    "    dx[range(num_x),y] -= 1\n",
    "    dx = dx / num_x\n",
    "\n",
    "    \n",
    "    return loss, dx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d81f6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class FullyConnectedNet(object):\n",
    "    \"\"\"Class for a multi-layer fully connected neural network.\n",
    "\n",
    "    Network contains an arbitrary number of hidden layers, ReLU nonlinearities,\n",
    "    and a softmax loss function. This will also implement dropout and batch/layer\n",
    "    normalization as options. For a network with L layers, the architecture will be\n",
    "\n",
    "    {affine - [batch/layer norm] - relu - [dropout]} x (L - 1) - affine - softmax\n",
    "\n",
    "    where batch/layer normalization and dropout are optional and the {...} block is\n",
    "    repeated L - 1 times.\n",
    "\n",
    "    Learnable parameters are stored in the self.params dictionary and will be learned\n",
    "    using the Solver class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dims,\n",
    "        input_dim=3 * 32 * 32,\n",
    "        num_classes=10,\n",
    "        dropout_keep_ratio=1,\n",
    "        normalization=None,\n",
    "        reg=0.0,\n",
    "        weight_scale=1e-2,\n",
    "        dtype=torch.float,\n",
    "        seed=None,\n",
    "    ):\n",
    "        \"\"\"Initialize a new FullyConnectedNet.\n",
    "\n",
    "        Inputs:\n",
    "        - hidden_dims: A list of integers giving the size of each hidden layer.\n",
    "        - input_dim: An integer giving the size of the input.\n",
    "        - num_classes: An integer giving the number of classes to classify.\n",
    "        - dropout_keep_ratio: Scalar between 0 and 1 giving dropout strength.\n",
    "            If dropout_keep_ratio=1 then the network should not use dropout at all.\n",
    "        - normalization: What type of normalization the network should use. Valid values\n",
    "            are \"batchnorm\", \"layernorm\", or None for no normalization (the default).\n",
    "        - reg: Scalar giving L2 regularization strength.\n",
    "        - weight_scale: Scalar giving the standard deviation for random\n",
    "            initialization of the weights.\n",
    "        - dtype: A numpy datatype object; all computations will be performed using\n",
    "            this datatype. float32 is faster but less accurate, so you should use\n",
    "            float64 for numeric gradient checking.\n",
    "        - seed: If not None, then pass this random seed to the dropout layers.\n",
    "            This will make the dropout layers deteriminstic so we can gradient check the model.\n",
    "        \"\"\"\n",
    "        self.normalization = normalization\n",
    "        self.use_dropout = dropout_keep_ratio != 1\n",
    "        self.reg = reg\n",
    "        self.num_layers = 1 + len(hidden_dims)\n",
    "        self.dtype = dtype\n",
    "        self.params = {}\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "        pass\n",
    "        self.params['W1'] = torch.normal(mean=0, std=weight_scale, size=(input_dim,hidden_dims[0]))\n",
    "        self.params['b1'] = torch.zeros((hidden_dims[0],))\n",
    "        for i in range(1,self.num_layers):\n",
    "            if i == self.num_layers-1:\n",
    "                self.params['W'+str(i+1)] = torch.normal(mean=0, std=weight_scale, size=(hidden_dims[i-1],num_classes))\n",
    "                self.params['b'+str(i+1)] = torch.zeros((num_classes,))\n",
    "            else:\n",
    "                self.params['W'+str(i+1)] = torch.normal(mean=0, std=weight_scale, size=(hidden_dims[i-1],hidden_dims[i]))\n",
    "                self.params['b'+str(i+1)] = torch.zeros((hidden_dims[i],))\n",
    "\n",
    "        if self.normalization == 'batchnorm' or self.normalization == 'layernorm':\n",
    "            for j in range(1,self.num_layers):\n",
    "                self.params['gamma'+str(j)] = torch.ones(hidden_dims[j-1])\n",
    "                self.params['beta'+str(j)] = torch.zeros(hidden_dims[j-1])\n",
    "\n",
    "        \n",
    "\n",
    "        # When using dropout we need to pass a dropout_param dictionary to each\n",
    "        # dropout layer so that the layer knows the dropout probability and the mode\n",
    "        # (train / test). You can pass the same dropout_param to each dropout layer.\n",
    "        self.dropout_param = {}\n",
    "        if self.use_dropout:\n",
    "            self.dropout_param = {\"mode\": \"train\", \"p\": dropout_keep_ratio}\n",
    "            if seed is not None:\n",
    "                self.dropout_param[\"seed\"] = seed\n",
    "\n",
    "        # With batch normalization we need to keep track of running means and\n",
    "        # variances, so we need to pass a special bn_param object to each batch\n",
    "        # normalization layer. You should pass self.bn_params[0] to the forward pass\n",
    "        # of the first batch normalization layer, self.bn_params[1] to the forward\n",
    "        # pass of the second batch normalization layer, etc.\n",
    "        self.bn_params = []\n",
    "        if self.normalization == \"batchnorm\":\n",
    "            self.bn_params = [{\"mode\": \"train\"} for i in range(self.num_layers - 1)]\n",
    "        if self.normalization == \"layernorm\":\n",
    "            self.bn_params = [{} for i in range(self.num_layers - 1)]\n",
    "\n",
    "        # Cast all parameters to the correct datatype.\n",
    "        for k, v in self.params.items():\n",
    "            self.params[k] = v.to(dtype)\n",
    "        \n",
    "    def save(self, path):\n",
    "        checkpoint = {\n",
    "          'reg': self.reg,\n",
    "          'dtype': self.dtype,\n",
    "          'params': self.params,\n",
    "          'num_layers': self.num_layers,\n",
    "          'use_dropout': self.use_dropout,\n",
    "          'dropout_param': self.dropout_param,\n",
    "        }\n",
    "\n",
    "        torch.save(checkpoint, path)\n",
    "        print(\"Saved in {}\".format(path))\n",
    "        \n",
    "    def load(self, path, dtype, device):\n",
    "        checkpoint = torch.load(path, map_location='cpu')\n",
    "        self.params = checkpoint['params']\n",
    "        self.dtype = dtype\n",
    "        self.reg = checkpoint['reg']\n",
    "        self.num_layers = checkpoint['num_layers']\n",
    "        self.use_dropout = checkpoint['use_dropout']\n",
    "        self.dropout_param = checkpoint['dropout_param']\n",
    "\n",
    "        for p in self.params:\n",
    "            self.params[p] = self.params[p].type(dtype).to(device)\n",
    "\n",
    "        print(\"load checkpoint file: {}\".format(path))\n",
    "        \n",
    "    def loss(self, X, y=None):\n",
    "        \"\"\"Compute loss and gradient for the fully connected net.\n",
    "        \n",
    "        Inputs:\n",
    "        - X: Array of input data of shape (N, d_1, ..., d_k)\n",
    "        - y: Array of labels, of shape (N,). y[i] gives the label for X[i].\n",
    "\n",
    "        Returns:\n",
    "        If y is None, then run a test-time forward pass of the model and return:\n",
    "        - scores: Array of shape (N, C) giving classification scores, where\n",
    "            scores[i, c] is the classification score for X[i] and class c.\n",
    "\n",
    "        If y is not None, then run a training-time forward and backward pass and\n",
    "        return a tuple of:\n",
    "        - loss: Scalar value giving the loss\n",
    "        - grads: Dictionary with the same keys as self.params, mapping parameter\n",
    "            names to gradients of the loss with respect to those parameters.\n",
    "        \"\"\"\n",
    "        \n",
    "        mode = \"test\" if y is None else \"train\"\n",
    "\n",
    "        # Set train/test mode for batchnorm params and dropout param since they\n",
    "        # behave differently during training and testing.\n",
    "        if self.use_dropout:\n",
    "            self.dropout_param[\"mode\"] = mode\n",
    "        if self.normalization == \"batchnorm\":\n",
    "            for bn_param in self.bn_params:\n",
    "                bn_param[\"mode\"] = mode\n",
    "        scores = None\n",
    "        \n",
    "\n",
    "        pass\n",
    "        scores = X\n",
    "        Cache = {}\n",
    "        for i in range(self.num_layers):\n",
    "            scores, Cache['af_'+str(i+1)] = affine_forward(scores,self.params['W'+str(i+1)],self.params['b'+str(i+1)])\n",
    "            if i != self.num_layers-1:\n",
    "                if self.normalization == 'batchnorm':\n",
    "                    scores, Cache['bn_'+str(i+1)] = batchnorm_forward(scores,\n",
    "                                                                        self.params['gamma'+str(i+1)],\n",
    "                                                                        self.params['beta'+str(i+1)],\n",
    "                                                                        self.bn_params[i])\n",
    "                elif self.normalization == 'layernorm':\n",
    "                    scores, Cache['ln_'+str(i+1)] = layernorm_forward(scores,\n",
    "                                                                        self.params['gamma'+str(i+1)],\n",
    "                                                                        self.params['beta'+str(i+1)],\n",
    "                                                                        self.bn_params[i])\n",
    "                scores, Cache['rl_'+str(i+1)] = relu_forward(scores)  # relu\n",
    "                if self.use_dropout:\n",
    "                    scores, Cache['do_'+str(i+1)] = dropout_forward(scores, self.dropout_param)\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        # If test mode return early.\n",
    "        if mode == \"test\":\n",
    "            return scores\n",
    "\n",
    "        loss, grads = 0.0, {}\n",
    "        \n",
    "\n",
    "        pass\n",
    "        loss,dx = softmax_loss2(scores, y)\n",
    "        for i in range(self.num_layers):\n",
    "            loss += 0.5 * self.reg * torch.sum(torch.pow(self.params['W'+str(i+1)],2))  # never forget the regularization\n",
    "        \n",
    "        for j in reversed(range(self.num_layers)):\n",
    "            if j != self.num_layers-1:\n",
    "                if self.use_dropout:\n",
    "                    dx = dropout_backward(dx,Cache['do_'+str(j+1)])\n",
    "                dx = relu_backwardd(dx,Cache['rl_'+str(j+1)])\n",
    "                if self.normalization == 'batchnorm':\n",
    "                    dx, grads['gamma'+str(j+1)],grads['beta'+str(j+1)] = batchnorm_backward(dx,Cache['bn_'+str(j+1)])\n",
    "                if self.normalization == 'layernorm':\n",
    "                    dx, grads['gamma'+str(j+1)],grads['beta'+str(j+1)] = layernorm_backward(dx,Cache['ln_'+str(j+1)])\n",
    "            dx,grads['W'+str(j+1)],grads['b'+str(j+1)] = affine_backward(dx,Cache['af_'+str(j+1)])\n",
    "            grads['W'+str(j+1)] += self.reg * self.params['W'+str(j+1)]\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        return loss, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63341669",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importnb import imports\n",
    "import importlib\n",
    "with imports(\"ipynb\"):\n",
    "    solver = importlib.import_module(\"solver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76816e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from solver import Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e3a2e49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_solver_instance(data_dict, dtype, device):\n",
    "    model = TwoLayerNet(hidden_dim=200, dtype=dtype, device=device)\n",
    "\n",
    "    solver = None\n",
    "\n",
    "    solver = Solver(model, data_dict)\n",
    "\n",
    "    return solver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8194b8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def get_three_layer_network_params():\n",
    "    \n",
    "        weight_scale = 2e-2   # Experiment with this!\n",
    "        learning_rate = 1e-2  # Experiment with this!\n",
    "\n",
    "        return weight_scale, learning_rate\n",
    "\n",
    "    def get_five_layer_network_params():\n",
    "    \n",
    "        learning_rate = 2e-2  # Experiment with this!\n",
    "        weight_scale = 5e-2   # Experiment with this!\n",
    "\n",
    "        return weight_scale, learning_rate\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da077fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(w, dw, config=None):\n",
    "    \"\"\"\n",
    "    Performs vanilla stochastic gradient descent.\n",
    "\n",
    "    config format:\n",
    "    - learning_rate: Scalar learning rate.\n",
    "    \"\"\"\n",
    "    if config is None: config = {}\n",
    "    config.setdefault('learning_rate', 1e-1)\n",
    "\n",
    "    w -= config['learning_rate'] * dw\n",
    "    return w, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d3e7dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def sgd_momentum(w, dw, config=None):\n",
    "    \"\"\"\n",
    "    Performs stochastic gradient descent with momentum.\n",
    "\n",
    "    config format:\n",
    "    - learning_rate: Scalar learning rate.\n",
    "    - momentum: Scalar between 0 and 1 giving the momentum value.\n",
    "      Setting momentum = 0 reduces to sgd.\n",
    "    - velocity: A numpy array of the same shape as w and dw used to store a\n",
    "      moving average of the gradients.\n",
    "    \"\"\"\n",
    "    if config is None: config = {}\n",
    "    config.setdefault('learning_rate', 1e-2)\n",
    "    config.setdefault('momentum', 0.9)\n",
    "    v = config.get('velocity', torch.zeros_like(w))\n",
    "\n",
    "    next_w = None\n",
    "    \n",
    "\n",
    "    v = config['momentum'] * v - config['learning_rate'] * dw\n",
    "    next_w = w + v\n",
    "\n",
    "    \n",
    "    config['velocity'] = v\n",
    "\n",
    "    return next_w, config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "607ddd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsprop(w, dw, config=None):\n",
    "    \"\"\"\n",
    "    Uses the RMSProp update rule, which uses a moving average of squared\n",
    "    gradient values to set adaptive per-parameter learning rates.\n",
    "\n",
    "    config format:\n",
    "    - learning_rate: Scalar learning rate.\n",
    "    - decay_rate: Scalar between 0 and 1 giving the decay rate for the squared\n",
    "      gradient cache.\n",
    "    - epsilon: Small scalar used for smoothing to avoid dividing by zero.\n",
    "    - cache: Moving average of second moments of gradients.\n",
    "    \"\"\"\n",
    "    if config is None: config = {}\n",
    "    config.setdefault('learning_rate', 1e-2)\n",
    "    config.setdefault('decay_rate', 0.99)\n",
    "    config.setdefault('epsilon', 1e-8)\n",
    "    config.setdefault('cache', torch.zeros_like(w))\n",
    "\n",
    "    next_w = None\n",
    "    \n",
    "\n",
    "    config['cache'] = config['decay_rate'] * config['cache'] + (1 - config['decay_rate']) * dw ** 2\n",
    "    next_w = w - config['learning_rate'] * dw / (torch.sqrt(config['cache']) + config['epsilon'])\n",
    "\n",
    "   \n",
    "\n",
    "    return next_w, config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a309dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def adam(w, dw, config=None):\n",
    "    \"\"\"\n",
    "    Uses the Adam update rule, which incorporates moving averages of both the\n",
    "    gradient and its square and a bias correction term.\n",
    "\n",
    "    config format:\n",
    "    - learning_rate: Scalar learning rate.\n",
    "    - beta1: Decay rate for moving average of first moment of gradient.\n",
    "    - beta2: Decay rate for moving average of second moment of gradient.\n",
    "    - epsilon: Small scalar used for smoothing to avoid dividing by zero.\n",
    "    - m: Moving average of gradient.\n",
    "    - v: Moving average of squared gradient.\n",
    "    - t: Iteration number.\n",
    "    \"\"\"\n",
    "    if config is None: config = {}\n",
    "    config.setdefault('learning_rate', 1e-3)\n",
    "    config.setdefault('beta1', 0.9)\n",
    "    config.setdefault('beta2', 0.999)\n",
    "    config.setdefault('epsilon', 1e-8)\n",
    "    config.setdefault('m', torch.zeros_like(w))\n",
    "    config.setdefault('v', torch.zeros_like(w))\n",
    "    config.setdefault('t', 0)\n",
    "\n",
    "    next_w = None\n",
    "    \n",
    "\n",
    "    config['m'] = config['beta1'] * config['m'] + (1 - config['beta1']) * dw\n",
    "    config['v'] = config['beta2'] * config['v'] + (1 - config['beta2']) * (dw ** 2)\n",
    "    next_w = w - config['learning_rate'] * config['m'] / (torch.sqrt(config['v']) + config['epsilon'])\n",
    "\n",
    "\n",
    "    return next_w, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ffcf616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class Dropout(object):\n",
    "    def forward(x, dropout_param):  \n",
    "        p, mode = dropout_param[\"p\"], dropout_param[\"mode\"]\n",
    "        if \"seed\" in dropout_param:\n",
    "            torch.manual_seed(dropout_param[\"seed\"])  \n",
    "        mask = None\n",
    "        out = None\n",
    "        if mode == \"train\":            \n",
    "            pass\n",
    "            mask = torch.bernoulli(torch.ones(*x.shape) * (1-p)) / (1-p)\n",
    "            out = mask * x      \n",
    "        elif mode == \"test\":\n",
    "            pass\n",
    "            out = x\n",
    "        cache = (dropout_param, mask)\n",
    "        out = out.to(x.dtype)\n",
    "        return out, cache\n",
    "\n",
    "    def backward(dout, cache):   \n",
    "        dropout_param, mask = cache\n",
    "        mode = dropout_param[\"mode\"]\n",
    "        dx = None\n",
    "        if mode == \"train\":        \n",
    "            pass\n",
    "            dx = dout * mask\n",
    "        elif mode == \"test\":\n",
    "            dx = dout\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c63172",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
